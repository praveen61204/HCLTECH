{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNajBBOL+ojf8gsdwhEavgD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praveen61204/HCLTECH/blob/master/assignment/paper2/paper2_assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJZnPmTJzl21",
        "outputId": "6583ee98-e2bb-4a30-fb10-1bc426429330"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            " Ths is an exampel txt corpus. It has sme mistakes, bad formating and impropr sentnces!!\n",
            "Natural languege processin is very importnt for AI, ML and data scince applcatons.\n",
            "\n",
            "Tokeniztion helps in breakng down the text. Lemmatiztion and steming also hlps.\n",
            "This txt contains multiple lines ,    weird spacings, and som wrong spellings.\n",
            "\n",
            "the cat siting on the mat was lookng at the brd. Meanwhile the dog dog was runing behind the carrr.\n",
            "\n",
            "how many sentnces are here?? maybe 3 Or 4 I am not sure. Let's see...!!\n",
            "\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "First 30 Tokens:\n",
            "['Ths', 'is', 'an', 'exampel', 'txt', 'corpus', '.', 'It', 'has', 'sme', 'mistakes', ',', 'bad', 'formating', 'and', 'impropr', 'sentnces', '!', '!', 'Natural', 'languege', 'processin', 'is', 'very', 'importnt', 'for', 'AI', ',', 'ML', 'and']\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "First 10 Corrected Tokens:\n",
            "['The', 'is', 'an', 'example', 'txt', 'corpus', '.', 'It', 'has', 'sme']\n",
            "\n",
            "Corrected Text Corpus:\n",
            "The is an example txt corpus . It has sme mistakes , bad formatting and improper sentences ! ! Natural language processing is very important for A , ML and data since applications . Tokeniztion helps in breaking down the text . Lemmatiztion and stemming also helps . This txt contains multiple lines , weird spacing , and som wrong spellings . the cat citing on the mat was looking at the brd . Meanwhile the dog dog was running behind the carry . how many sentences are here ? ? maybe 3 Or 4 I am not sure . Let 's see ... ! !\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "POS Tags for Corrected Tokens:\n",
            "[('The', 'DT'), ('is', 'VBZ'), ('an', 'DT'), ('example', 'NN'), ('txt', 'NN'), ('corpus', 'NN'), ('.', '.'), ('It', 'PRP'), ('has', 'VBZ'), ('sme', 'VBN'), ('mistakes', 'NNS'), (',', ','), ('bad', 'JJ'), ('formatting', 'NN'), ('and', 'CC'), ('improper', 'JJ'), ('sentences', 'NNS'), ('!', '.'), ('!', '.'), ('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('is', 'VBZ'), ('very', 'RB'), ('important', 'JJ'), ('for', 'IN'), ('A', 'NNP'), (',', ','), ('ML', 'NNP'), ('and', 'CC')]\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "First 20 Tokens After Stopword Removal:\n",
            "['example', 'txt', 'corpus', '.', 'sme', 'mistakes', ',', 'bad', 'formatting', 'improper', 'sentences', '!', '!', 'Natural', 'language', 'processing', 'important', ',', 'ML', 'data']\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "First 20 Stemmed Tokens:\n",
            "['exampl', 'txt', 'corpu', '.', 'sme', 'mistak', ',', 'bad', 'format', 'improp', 'sentenc', '!', '!', 'natur', 'languag', 'process', 'import', ',', 'ml', 'data']\n",
            "\n",
            "First 20 Lemmatized Tokens:\n",
            "['example', 'txt', 'corpus', '.', 'sme', 'mistake', ',', 'bad', 'formatting', 'improper', 'sentence', '!', '!', 'Natural', 'language', 'processing', 'important', ',', 'ML', 'data']\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "Total Number of Sentences: 12\n",
            "\n",
            "Sentences:\n",
            "['Ths is an exampel txt corpus.', 'It has sme mistakes, bad formating and impropr sentnces!!', 'Natural languege processin is very importnt for AI, ML and data scince applcatons.', 'Tokeniztion helps in breakng down the text.', 'Lemmatiztion and steming also hlps.', 'This txt contains multiple lines ,    weird spacings, and som wrong spellings.', 'the cat siting on the mat was lookng at the brd.', 'Meanwhile the dog dog was runing behind the carrr.', 'how many sentnces are here??', 'maybe 3 Or 4 I am not sure.', \"Let's see...!\", '!']\n"
          ]
        }
      ],
      "source": [
        "#NLTK\n",
        "\n",
        "# a) Import necessary libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from autocorrect import Speller\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Download required NLTK data (run only once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# b) Load the text corpus\n",
        "with open(\"file.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"Original Text:\\n\", text)\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "\n",
        "# c) Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "print(\"\\nFirst 30 Tokens:\")\n",
        "print(tokens[:30])\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "\n",
        "# d) Spelling correction\n",
        "spell = Speller(lang='en')\n",
        "corrected_tokens = [spell(token) for token in tokens]\n",
        "\n",
        "print(\"\\nFirst 10 Corrected Tokens:\")\n",
        "print(corrected_tokens[:10])\n",
        "\n",
        "corrected_text = \" \".join(corrected_tokens)\n",
        "print(\"\\nCorrected Text Corpus:\")\n",
        "print(corrected_text)\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "\n",
        "# e) POS Tagging\n",
        "pos_tags = nltk.pos_tag(corrected_tokens)\n",
        "\n",
        "print(\"\\nPOS Tags for Corrected Tokens:\")\n",
        "print(pos_tags[:30])\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "\n",
        "# f) Stopword Removal\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_tokens = [tok for tok in corrected_tokens if tok.lower() not in stop_words]\n",
        "\n",
        "print(\"\\nFirst 20 Tokens After Stopword Removal:\")\n",
        "print(filtered_tokens[:20])\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "\n",
        "# g) Stemming and Lemmatization\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "stemmed = [stemmer.stem(tok) for tok in filtered_tokens]\n",
        "lemmatized = [lemmatizer.lemmatize(tok) for tok in filtered_tokens]\n",
        "\n",
        "print(\"\\nFirst 20 Stemmed Tokens:\")\n",
        "print(stemmed[:20])\n",
        "\n",
        "print(\"\\nFirst 20 Lemmatized Tokens:\")\n",
        "print(lemmatized[:20])\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "\n",
        "# h) Sentence Boundary Detection\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"\\nTotal Number of Sentences:\", len(sentences))\n",
        "print(\"\\nSentences:\")\n",
        "print(sentences)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Import the necessary packages\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import re\n",
        "\n",
        "# b) Fetch the dataset and store in a DataFrame\n",
        "newsgroups = fetch_20newsgroups(subset='all',\n",
        "                                remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"text\": newsgroups.data,\n",
        "    \"target\": newsgroups.target\n",
        "})\n",
        "\n",
        "print(\"Dataset Loaded!\")\n",
        "print(df.head())\n",
        "print(\"\\nNumber of documents:\", len(df))\n",
        "\n",
        "# c) Clean the text data\n",
        "def clean_text(text):\n",
        "    text = text.lower()                                  # Lowercase\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)                 # Remove punctuation\n",
        "    text = re.sub(r'\\d+', ' ', text)                     # Remove numbers\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()             # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "\n",
        "print(\"\\nCleaned Text Sample:\")\n",
        "print(df[\"clean_text\"].iloc[0])\n",
        "\n",
        "# d) Create a Bag-of-Words (BoW) model\n",
        "bow_vectorizer = CountVectorizer(stop_words='english')\n",
        "bow_matrix = bow_vectorizer.fit_transform(df[\"clean_text\"])\n",
        "\n",
        "print(\"\\nBoW Shape:\", bow_matrix.shape)\n",
        "\n",
        "# Sum frequency of each word\n",
        "bow_word_counts = bow_matrix.sum(axis=0).A1\n",
        "bow_vocab = bow_vectorizer.get_feature_names_out()\n",
        "\n",
        "bow_freq_df = pd.DataFrame({\n",
        "    \"word\": bow_vocab,\n",
        "    \"count\": bow_word_counts\n",
        "}).sort_values(by=\"count\", ascending=False)\n",
        "\n",
        "print(\"\\nTop 20 words (BoW):\")\n",
        "print(bow_freq_df.head(20))\n",
        "\n",
        "# e) Create a TF-IDF model\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df[\"clean_text\"])\n",
        "\n",
        "print(\"\\nTF-IDF Shape:\", tfidf_matrix.shape)\n",
        "\n",
        "# Sum TF-IDF scores for each word\n",
        "tfidf_word_scores = tfidf_matrix.sum(axis=0).A1\n",
        "tfidf_vocab = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "tfidf_freq_df = pd.DataFrame({\n",
        "    \"word\": tfidf_vocab,\n",
        "    \"score\": tfidf_word_scores\n",
        "}).sort_values(by=\"score\", ascending=False)\n",
        "\n",
        "print(\"\\nTop 20 words (TF-IDF):\")\n",
        "print(tfidf_freq_df.head(20))\n",
        "\n",
        "# f) Compare both models\n",
        "print(\"\\n================= COMPARISON =================\")\n",
        "print(\"\\nTop 20 BoW Words:\")\n",
        "print(list(bow_freq_df.head(20)[\"word\"]))\n",
        "\n",
        "print(\"\\nTop 20 TF-IDF Words:\")\n",
        "print(list(tfidf_freq_df.head(20)[\"word\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXWzlpr81XF1",
        "outputId": "af348b58-06dc-40b0-d987-c639aff3ca20"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Loaded!\n",
            "                                                text  target\n",
            "0  \\n\\nI am sure some bashers of Pens fans are pr...      10\n",
            "1  My brother is in the market for a high-perform...       3\n",
            "2  \\n\\n\\n\\n\\tFinally you said what you dream abou...      17\n",
            "3  \\nThink!\\n\\nIt's the SCSI card doing the DMA t...       3\n",
            "4  1)    I have an old Jasmine drive which I cann...       4\n",
            "\n",
            "Number of documents: 18846\n",
            "\n",
            "Cleaned Text Sample:\n",
            "i am sure some bashers of pens fans are pretty confused about the lack of any kind of posts about the recent pens massacre of the devils actually i am bit puzzled too and a bit relieved however i am going to put an end to non pittsburghers relief with a bit of praise for the pens man they are killing those devils worse than i thought jagr just showed you why he is much better than his regular season stats he is also a lot fo fun to watch in the playoffs bowman should let jagr have a lot of fun in the next couple of games since the pens are going to beat the pulp out of jersey anyway i was very disappointed not to see the islanders lose the final regular season game pens rule\n",
            "\n",
            "BoW Shape: (18846, 97871)\n",
            "\n",
            "Top 20 words (BoW):\n",
            "         word  count\n",
            "7280       ax  62502\n",
            "48213    like   6525\n",
            "23285     don   6524\n",
            "62890  people   6458\n",
            "44115    just   6172\n",
            "45821    know   5763\n",
            "88732     use   5029\n",
            "83813   think   5001\n",
            "84253    time   4867\n",
            "51317     max   4657\n",
            "23181    does   4505\n",
            "57248     new   4089\n",
            "33734    good   3921\n",
            "24920     edu   3754\n",
            "33587     god   3399\n",
            "91712     way   3398\n",
            "50397    make   3349\n",
            "89628      ve   3184\n",
            "88736    used   3048\n",
            "74257     say   3015\n",
            "\n",
            "TF-IDF Shape: (18846, 97871)\n",
            "\n",
            "Top 20 words (TF-IDF):\n",
            "          word       score\n",
            "48213     like  244.287673\n",
            "44115     just  242.959197\n",
            "23285      don  240.834665\n",
            "45821     know  240.447598\n",
            "62890   people  211.440538\n",
            "83813    think  203.435264\n",
            "23181     does  200.956131\n",
            "88732      use  175.597055\n",
            "83528   thanks  172.842043\n",
            "33734     good  168.258826\n",
            "84253     time  167.418508\n",
            "57248      new  151.851737\n",
            "89628       ve  150.334274\n",
            "33587      god  136.347896\n",
            "50397     make  133.911756\n",
            "91712      way  133.279740\n",
            "56873     need  132.709528\n",
            "92682  windows  132.399184\n",
            "21770      did  130.872639\n",
            "91480     want  130.797029\n",
            "\n",
            "================= COMPARISON =================\n",
            "\n",
            "Top 20 BoW Words:\n",
            "['ax', 'like', 'don', 'people', 'just', 'know', 'use', 'think', 'time', 'max', 'does', 'new', 'good', 'edu', 'god', 'way', 'make', 've', 'used', 'say']\n",
            "\n",
            "Top 20 TF-IDF Words:\n",
            "['like', 'just', 'don', 'know', 'people', 'think', 'does', 'use', 'thanks', 'good', 'time', 'new', 've', 'god', 'make', 'way', 'need', 'windows', 'did', 'want']\n"
          ]
        }
      ]
    }
  ]
}